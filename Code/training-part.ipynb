{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"## Part One Feature Engineering","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport os, gc, random\nimport numpy as np, pandas as pd\nfrom numpy.fft import rfft, rfftfreq\n\nGOOD_CSV  = \"/kaggle/input/nasa-bearing-dataset/train_final.csv\"   # raw healthy data\nFAULT_CSV = \"/kaggle/working/merged_test.csv\"  # raw faulty data\nOUT_DIR   = \"/kaggle/working/features\"; os.makedirs(OUT_DIR, exist_ok=True)\n\nWIN  = 2048\nHOP  = 512\nFFT_N = 1024\nEPS  = 1e-9\n\n\ndef robust_norm(x):\n    med = np.median(x); mad = np.median(np.abs(x - med)) + EPS\n    z = (x - med) / (1.4826 * mad)\n    return np.clip(z, -6.0, 6.0)\n\ndef time_features(x):\n    mean = float(np.mean(x))\n    std  = float(np.std(x) + EPS)\n    rms  = float(np.sqrt(np.mean(x*x)))\n    mad  = float(np.mean(np.abs(x - mean)))\n    p2p  = float(np.max(x) - np.min(x))\n    skew = float(np.mean(((x-mean)/std)**3))\n    kurt = float(np.mean(((x-mean)/std)**4))\n    crest= float(np.max(np.abs(x)) / (rms + EPS))\n    imp  = float((np.max(np.abs(x)) + EPS) / (np.mean(np.abs(x)) + EPS))\n    return [mean,std,rms,mad,p2p,skew,kurt,crest,imp]\n\ndef freq_features(x, fs=1.0, nfft=FFT_N):\n    if len(x) < nfft:\n        pad = np.zeros(nfft, dtype=np.float32); pad[:len(x)] = x\n        x = pad\n    X = np.abs(rfft(x))\n    X = X / (np.sum(X) + EPS)\n    freqs = rfftfreq(len(x), d=1.0/fs)\n    centroid = float(np.sum(freqs * X))\n    bandwidth = float(np.sqrt(np.sum(((freqs-centroid)**2)*X)))\n    csum = np.cumsum(X); idx = np.searchsorted(csum, 0.95)\n    rolloff = float(freqs[min(idx, len(freqs)-1)])\n    geo = np.exp(np.mean(np.log(X + EPS))); arith = np.mean(X) + EPS\n    flatness = float(geo/arith)\n    entropy = float(-np.sum(X * np.log(X + EPS)))\n    dom = float(freqs[np.argmax(X)])\n    top2 = np.partition(X, -2)[-2:]\n    peak_ratio = float((top2[-1]+EPS)/(top2[-2]+EPS))\n    q = len(X)//4\n    bands = [float(np.sum(X[i*q:(i+1)*q])) for i in range(4)]\n    return [centroid, bandwidth, rolloff, flatness, entropy, dom, peak_ratio] + bands\n\ndef window_iter(buf):\n    n = buf.shape[0]\n    for s in range(0, n - WIN + 1, HOP):\n        yield buf[s:s+WIN]\n\ndef detect_bearing_cols(df):\n    cols = [c for c in df.columns if c.lower().startswith(\"bearing\")]\n    if len(cols) < 4:\n        raise ValueError(f\"Expected ≥4 bearing columns, got {cols}\")\n    return cols[:4]\n\ndef process_csv(path, tag):\n    feats = []\n    prev_tail = None\n    for df in pd.read_csv(path, chunksize=2_000_000):\n        cols = detect_bearing_cols(df)\n        X = df[cols].astype(np.float32).values\n        if prev_tail is not None:\n            X = np.vstack([prev_tail, X])\n        for win in window_iter(X):\n            if np.all(np.var(win, axis=0) < 1e-8): \n                continue\n            fvec = []\n            for ch in range(win.shape[1]):\n                x = robust_norm(win[:, ch])\n                fvec += time_features(x) + freq_features(x)\n            feats.append(fvec)\n        prev_tail = X[-(WIN-HOP):].copy()\n        del df, X; gc.collect()\n    df_out = pd.DataFrame(feats)\n    out_path = os.path.join(OUT_DIR, f\"features_{tag}.csv\")\n    df_out.to_csv(out_path, index=False)\n    print(f\"Saved {len(df_out)} windows to {out_path}\")\n\nif __name__ == \"__main__\":\n    process_csv(GOOD_CSV, \"good\")\n    process_csv(FAULT_CSV, \"fault\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Part two, training\nENABLED = {\"AE\": False, \"SEN\": False, \"VAE\": True}\nSAVE_WEIGHTS = {\"AE\": False, \"SEN\": False, \"VAE\": True}\nEXPORT_VAE = True\n\n\nimport os\nSEED_MASTER = 13\nos.environ[\"PYTHONHASHSEED\"] = str(SEED_MASTER)\nos.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\"\nos.environ[\"TF_CUDNN_DETERMINISTIC\"] = \"1\"\nos.environ[\"OMP_NUM_THREADS\"] = \"1\"\nos.environ[\"MKL_NUM_THREADS\"] = \"1\"\n\n\nimport json, random, gc, joblib\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom mpl_toolkits.mplot3d import Axes3D  #\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import (\n    roc_auc_score, average_precision_score,\n    roc_curve, precision_recall_curve, confusion_matrix\n)\nfrom sklearn.manifold import trustworthiness\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import RobustScaler\n\nFEAT_GOOD  = \"./features/features_good.csv\"\nFEAT_FAULT = \"./features/features_fault.csv\"\nOUT_DIR    = \"./outputs_toggle\"; os.makedirs(OUT_DIR, exist_ok=True)\n\n\nSEEDS       = [303]  ## pore lagle aro add hobe eikahne\n\n\nEPOCHS_AE   = 30\nEPOCHS_SEN  = 35\nEPOCHS_VAE  = 40\nBATCH       = 256\nHID         = [256, 128]\nLATENT      = 8\n\nwarm-up\nBETA_START, BETA_END, WARMUP_EPOCHS = 0.0, 1.0, 20\n\n\nSEN_NOISE_STD = 0.02  # training-time only\n\n\nN_NEIGH     = 10\nUSE_FLOAT16 = True\nMC_RUNS     = 5\nVIEW_ANGLES_3D = [\n    dict(elev=90, azim=200,  name=\"viewA\"),\n    dict(elev=30, azim=180,  name=\"viewB\"),\n]\n\n\nOUTPUT_LINEAR = False \n\n\ntf.config.threading.set_intra_op_parallelism_threads(1)\ntf.config.threading.set_inter_op_parallelism_threads(1)\ntry:\n    tf.keras.utils.set_random_seed(SEED_MASTER)\nexcept Exception:\n    random.seed(SEED_MASTER); np.random.seed(SEED_MASTER); tf.random.set_seed(SEED_MASTER)\n\n\ndef set_seed(s: int):\n    random.seed(s); np.random.seed(s); tf.random.set_seed(s)\n\ndef get_optimizer(lr=1e-3):\n    if hasattr(tf.keras.optimizers, \"AdamW\"):\n        return tf.keras.optimizers.AdamW(learning_rate=lr)\n    return tf.keras.optimizers.Adam(learning_rate=lr)\n\n\ndef load_and_scale(noise_std=0.005, use_robust=True):\n    Xg = pd.read_csv(FEAT_GOOD).values.astype(np.float32)   # healthy\n    Xf = pd.read_csv(FEAT_FAULT).values.astype(np.float32)  # faulty\n\n    if use_robust:\n        scaler = RobustScaler()\n        Xg = scaler.fit_transform(Xg)\n        Xf = scaler.transform(Xf)\n        scaler_info = scaler\n    else:\n        mn = np.percentile(Xg, 1, axis=0)\n        mx = np.percentile(Xg, 99, axis=0)\n        Xg = np.clip((Xg - mn)/(mx - mn + 1e-6), 0, 1)\n        Xf = np.clip((Xf - mn)/(mx - mn + 1e-6), 0, 1)\n        scaler_info = dict(mn=mn, mx=mx)\n\n    rng = np.random.default_rng(42)\n    Xg = Xg + rng.normal(0, noise_std, Xg.shape).astype(np.float32)\n    if not use_robust:\n        Xg = np.clip(Xg, 0, 1)\n\n    if USE_FLOAT16:\n        Xg, Xf = Xg.astype(np.float16), Xf.astype(np.float16)\n    return Xg, Xf, scaler_info\n\ndef make_ds(X, batch, shuffle=False, seed=None):\n    ds = tf.data.Dataset.from_tensor_slices((X, X))\n    if shuffle:\n        ds = ds.shuffle(buffer_size=min(10000, len(X)),\n                        seed=seed, reshuffle_each_iteration=False)\n    opts = tf.data.Options()\n    opts.experimental_deterministic = True\n    ds = ds.with_options(opts)\n    return ds.batch(batch, drop_remainder=False).prefetch(1)\n\ndef recon_errors(model, X, bs=1024, deterministic=True):\n    errs = []\n    for i in range(0, len(X), bs):\n        xb = X[i:i+bs].astype(np.float32)\n        if hasattr(model, \"encode_mu\") and deterministic:\n            mu = model.encode_mu(xb)\n            xh = model.decode(mu).numpy()\n        else:\n            xh = model.predict(xb, verbose=0)\n            if hasattr(xh, \"numpy\"):\n                xh = xh.numpy()\n        errs.append(np.mean((xh - xb)**2, axis=1))\n    return np.concatenate(errs, 0)\n\ndef sample_for_manifold(X, max_n=10000, seed=123):\n    if len(X) <= max_n: return X\n    rng = np.random.default_rng(seed)\n    return X[rng.choice(len(X), size=max_n, replace=False)]\n\ndef thr_percentile(errs, p=99.5): return float(np.percentile(errs, p))\n\ndef thr_evt(errs, tail_p=5.0, q=1e-3):\n    u = np.percentile(errs, 100 - tail_p)\n    y = errs[errs >= u] - u\n    if len(y) < 200:\n        return thr_percentile(errs, 99.5)\n    y = np.sort(y); k = len(y)\n    x1, x2, x4 = y[int(0.25*k)], y[int(0.5*k)], y[int(0.75*k)]\n    xi = (np.log((x4 - x2) + 1e-9) - np.log((x2 - x1) + 1e-9)) / np.log(2)\n    xi = np.clip(xi, -0.49, 0.49)\n    beta = (1 + xi) * np.mean(y)\n    thr = u + (beta/xi) * ((q**(-xi)) - 1) if abs(xi) > 1e-6 else u - beta*np.log(q)\n    return float(thr)\n\n\ndef plot_hist(err_g, err_f, tag, pmax=99.5, use_log=False):\n \n    import numpy as np, matplotlib.pyplot as plt, os\n    eps = 1e-12\n    err_g = np.asarray(err_g, dtype=np.float64); err_g = err_g[np.isfinite(err_g)]\n    err_f = np.asarray(err_f, dtype=np.float64); err_f = err_f[np.isfinite(err_f)]\n    if use_log:\n        xg = np.log10(np.maximum(err_g, eps)); xf = np.log10(np.maximum(err_f, eps))\n        xlabel = \"log10(MSE)\"\n    else:\n        xg, xf = err_g, err_f\n        xlabel = \"MSE\"\n    lo = min(xg.min(initial=0.0), xf.min(initial=0.0))\n    hi = np.percentile(np.concatenate([xg, xf]), pmax)\n    if not np.isfinite(hi) or hi <= lo:\n        hi = max(xg.max(initial=1.0), xf.max(initial=1.0))\n    bins = np.linspace(lo, hi, 100)\n\n    plt.figure()\n    plt.hist(xg, bins=bins, density=True, alpha=0.6, label=\"Without Fault\")\n    plt.hist(xf, bins=bins, density=True, alpha=0.6, label=\"Near Faulty\")\n    plt.legend(); plt.xlabel(xlabel); plt.ylabel(\"pdf\")\n    plt.title(tag + (\" (robust)\" if not use_log else \" (log-scale)\"))\n    plt.tight_layout()\n    plt.savefig(os.path.join(OUT_DIR, f\"{tag}.png\"), dpi=150)\n    plt.close()\n    print(f\"[{tag}] {xlabel} range used: [{lo:.4g}, {hi:.4g}] \"\n          f\"| medians: H={np.median(xg):.4g}, F={np.median(xf):.4g}\")\n\n# -----------------------------------------------------------\ndef plot_roc_pr(y_true, scores, tag):\n    fpr, tpr, _ = roc_curve(y_true, scores)\n    prec, rec, _ = precision_recall_curve(y_true, scores)\n    plt.figure(); plt.plot(fpr, tpr); plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(f\"ROC {tag}\")\n    plt.savefig(os.path.join(OUT_DIR, f\"roc_{tag}.png\"), dpi=150); plt.close()\n    plt.figure(); plt.plot(rec, prec); plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(f\"PR {tag}\")\n    plt.savefig(os.path.join(OUT_DIR, f\"pr_{tag}.png\"), dpi=150); plt.close()\n\ndef _stabilize_pca_signs(pca: PCA):\n    comps = pca.components_.copy()\n    for i in range(comps.shape[0]):\n        j = np.argmax(np.abs(comps[i]))\n        if comps[i, j] < 0:\n            comps[i] *= -1.0\n    pca.components_ = comps\n    return pca\n\ndef _apply_pca_projection(mu, mean, comps):\n    return (mu - mean) @ comps.T\n\n\ndef save_latent_pca3_npz(mu_g, mu_f, out_npz_path):\n    pca3 = PCA(n_components=3, svd_solver=\"full\").fit(mu_g)\n    comps = pca3.components_.copy()\n    for i in range(comps.shape[0]):\n        j = np.argmax(np.abs(comps[i]))\n        if comps[i, j] < 0: comps[i] *= -1.0\n    mean_ref3 = pca3.mean_.astype(np.float32)\n    comps_ref3 = comps.astype(np.float32)\n    Zg3 = _apply_pca_projection(mu_g, mean_ref3, comps_ref3).astype(np.float32)\n    Zf3 = _apply_pca_projection(mu_f, mean_ref3, comps_ref3).astype(np.float32)\n    np.savez(out_npz_path,\n             Zg3=Zg3, Zf3=Zf3,\n             mean=mean_ref3, components=comps_ref3,\n             evr=pca3.explained_variance_ratio_.astype(np.float32))\n\ndef plot_latent(mu_g, mu_f, tag):\n    # 2D PCA\n    pca2 = PCA(n_components=2, svd_solver=\"full\")\n    Zg2 = pca2.fit_transform(mu_g)\n    pca2 = _stabilize_pca_signs(pca2)\n    mean_ref2 = pca2.mean_.copy(); comps_ref2 = pca2.components_.copy()\n    Zg2 = _apply_pca_projection(mu_g, mean_ref2, comps_ref2)\n    Zf2 = _apply_pca_projection(mu_f, mean_ref2, comps_ref2)\n    evr2 = pca2.explained_variance_ratio_.copy()\n    plt.figure()\n    plt.scatter(Zg2[:,0], Zg2[:,1], s=4, alpha=0.5, label=\"healthy\")\n    plt.scatter(Zf2[:,0], Zf2[:,1], s=4, alpha=0.6, label=\"faulty\")\n    plt.legend(); plt.title(f\"Latent PCA μ(z) – 2D ({tag})\")\n    plt.xlabel(\"PC1\"); plt.ylabel(\"PC2\")\n    plt.savefig(os.path.join(OUT_DIR, f\"latent_pca_2d_{tag}.png\"), dpi=150); plt.close()\n    \n    # 3D views pCA\n    pca3 = PCA(n_components=3, svd_solver=\"full\")\n    Zg3 = pca3.fit_transform(mu_g)\n    pca3 = _stabilize_pca_signs(pca3)\n    mean_ref3 = pca3.mean_.copy(); comps_ref3 = pca3.components_.copy()\n    Zg3 = _apply_pca_projection(mu_g, mean_ref3, comps_ref3)\n    Zf3 = _apply_pca_projection(mu_f, mean_ref3, comps_ref3)\n    for v in VIEW_ANGLES_3D:\n        fig = plt.figure()\n        ax = fig.add_subplot(111, projection='3d')\n        ax.scatter(Zg3[:,0], Zg3[:,1], Zg3[:,2], s=3, alpha=0.3, label=\"healthy\")\n        ax.scatter(Zf3[:,0], Zf3[:,1], Zf3[:,2], s=9, alpha=0.85, label=\"faulty\")\n        ax.set_xlabel(\"PC1\"); ax.set_ylabel(\"PC2\"); ax.set_zlabel(\"PC3\")\n        ax.legend(); ax.set_title(f\"Latent PCA μ(z) – 3D ({tag})\")\n        ax.view_init(elev=v[\"elev\"], azim=v[\"azim\"])\n        plt.savefig(os.path.join(OUT_DIR, f\"latent_pca_3d_{tag}_{v['name']}.png\"), dpi=150)\n        plt.close()\n    return evr2\n\ndef dense_param_count(a, b): return a*b + b\ndef dense_macs(a, b): return a*b\n\ndef ae_like_stats(inp, hidden, latent):\n    sizes = [inp] + hidden + [latent]\n    sizes_dec = [latent] + list(reversed(hidden)) + [inp]\n    p = sum(dense_param_count(a,b) for a,b in zip(sizes[:-1], sizes[1:]))\n    p += sum(dense_param_count(a,b) for a,b in zip(sizes_dec[:-1], sizes_dec[1:]))\n    m = sum(dense_macs(a,b) for a,b in zip(sizes[:-1], sizes[1:]))\n    m += sum(dense_macs(a,b) for a,b in zip(sizes_dec[:-1], sizes_dec[1:]))\n    return p, m\n\ndef vae_stats(inp, hidden, latent):\n    sizes_enc = [inp] + hidden\n    sizes_dec = [latent] + list(reversed(hidden)) + [inp]\n    p = sum(dense_param_count(a,b) for a,b in zip(sizes_enc[:-1], sizes_enc[1:]))\n    p += dense_param_count(hidden[-1], latent) * 2 \n    p += sum(dense_param_count(a,b) for a,b in zip(sizes_dec[:-1], sizes_dec[1:]))\n    m = sum(dense_macs(a,b) for a,b in zip(sizes_enc[:-1], sizes_enc[1:]))\n    m += dense_macs(hidden[-1], latent) * 2\n    m += sum(dense_macs(a,b) for a,b in zip(sizes_dec[:-1], sizes_dec[1:]))\n    return p, m\n\ndef estimate_latency_ms(macs, mmacs_per_s=50.0):\n    return (macs / (mmacs_per_s * 1e6)) * 1000.0\n\ndef pretty_stats(name, input_dim, hidden, latent, keras_model=None, tflite_path=None):\n    lname = name.lower()\n    if lname == \"vae\":\n        P, M = vae_stats(input_dim, hidden, latent)\n    else:\n        P, M = ae_like_stats(input_dim, hidden, latent)\n    kp = None\n    if keras_model is not None:\n        try: _ = keras_model(tf.zeros((1, input_dim), dtype=tf.float32))\n        except Exception: pass\n        try: kp = keras_model.count_params()\n        except: kp = None\n    fs = None\n    if tflite_path and os.path.exists(tflite_path):\n        fs = os.path.getsize(tflite_path) / 1024.0\n    print(f\"\\n[{name}] Params≈{P:,}\" + (f\" (Keras={kp:,})\" if kp is not None else \" (Keras=N/A)\"))\n    print(f\"  MACs≈{M:,} (~{M/1e9:.4f} GMACs), FLOPs≈{2*M/1e9:.4f} GFLOPs\")\n    if fs: print(f\"  TFLite size≈{fs:.1f} KiB\")\n    print(f\"  ~Latency≈{estimate_latency_ms(M, 50.0):.4f} ms @ 50 MMAC/s\")\n\nclass AE(tf.keras.Model):\n    def __init__(self, input_dim, hidden, latent):\n        super().__init__(name=\"AE\")\n        self.enc_layers = [tf.keras.layers.Dense(h, activation=\"relu\") for h in hidden]\n        self.mu = tf.keras.layers.Dense(latent, name=\"bottleneck\")\n        self.dec_layers = [tf.keras.layers.Dense(h, activation=\"relu\") for h in reversed(hidden)]\n        act = None if OUTPUT_LINEAR else \"sigmoid\"\n        self.out_layer  = tf.keras.layers.Dense(input_dim, activation=act)\n        self.mse = tf.keras.losses.MeanSquaredError()\n    def encode(self, x):\n        h = x\n        for l in self.enc_layers: h = l(h)\n        return self.mu(h)\n    def encode_mu(self, x):\n        x = tf.convert_to_tensor(x, dtype=tf.float32)\n        return self.encode(x)\n    def decode(self, z):\n        h = z\n        for l in self.dec_layers: h = l(h)\n        return self.out_layer(h)\n    def call(self, x, training=False):\n        return self.decode(self.encode(x))\n\ndef build_ae(input_dim):\n    ae = AE(input_dim, HID, LATENT)\n    ae.compile(optimizer=get_optimizer(1e-3), loss=\"mse\")\n    return ae\n\nclass SEN(tf.keras.Model):\n    def __init__(self, input_dim, hidden, latent, noise_std=0.02):\n        super().__init__(name=\"SEN\")\n        self.noise_std = noise_std\n        self.enc_layers = [tf.keras.layers.Dense(h, activation=\"relu\") for h in hidden]\n        self.mu = tf.keras.layers.Dense(latent)\n        self.dec_layers = [tf.keras.layers.Dense(h, activation=\"relu\") for h in reversed(hidden)]\n        act = None if OUTPUT_LINEAR else \"sigmoid\"\n        self.out_layer  = tf.keras.layers.Dense(input_dim, activation=act)\n        self.mse = tf.keras.losses.MeanSquaredError()\n    def encode(self, x, training=False):\n        h = x\n        for l in self.enc_layers: h = l(h)\n        mu = self.mu(h)\n        z = mu + tf.random.normal(tf.shape(mu), stddev=self.noise_std) if training and self.noise_std>0 else mu\n        return mu, z\n    def decode(self, z):\n        h = z\n        for l in self.dec_layers: h = l(h)\n        return self.out_layer(h)\n    def call(self, x, training=False):\n        _, z = self.encode(x, training=training)\n        return self.decode(z)\n    def train_step(self, data):\n        x = data[0] if isinstance(data, tuple) else data\n        with tf.GradientTape() as tape:\n            mu, z = self.encode(x, training=True)\n            xh = self.decode(z)\n            loss = self.mse(x, xh)\n        grads = tape.gradient(loss, self.trainable_variables)\n        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n        return {\"loss\": loss}\n    def test_step(self, data):\n        x = data[0] if isinstance(data, tuple) else data\n        mu, z = self.encode(x, training=False)\n        xh = self.decode(z)\n        loss = self.mse(x, xh)\n        return {\"loss\": loss}\n    def encode_mu(self, x):\n        x = tf.convert_to_tensor(x, dtype=tf.float32)\n        h = x\n        for l in self.enc_layers: h = l(h)\n        return self.mu(h)\n\ndef build_sen(input_dim, noise_std=SEN_NOISE_STD):\n    sen = SEN(input_dim, HID, LATENT, noise_std=noise_std)\n    sen.compile(optimizer=get_optimizer(1e-3))\n    return sen\n\nclass VAE(tf.keras.Model):\n    def __init__(self, input_dim, hidden, latent, beta=1.0, noise_std=0.01):\n        super().__init__(name=\"VAE\")\n        self.beta = beta\n        self.noise_std = noise_std\n        self.enc_layers = [tf.keras.layers.Dense(h, activation=\"relu\") for h in hidden]\n        self.mu      = tf.keras.layers.Dense(latent)\n        self.logvar  = tf.keras.layers.Dense(latent)\n        self.dec_layers = [tf.keras.layers.Dense(h, activation=\"relu\") for h in reversed(hidden)]\n        act = None if OUTPUT_LINEAR else \"sigmoid\"\n        self.out_layer  = tf.keras.layers.Dense(input_dim, activation=act)\n        self.mse = tf.keras.losses.MeanSquaredError()\n    def encode(self, x, training=False):\n        if training and self.noise_std>0: x = x + tf.random.normal(tf.shape(x), stddev=self.noise_std)\n        h = x\n        for l in self.enc_layers: h = l(h)\n        return self.mu(h), self.logvar(h)\n    def decode(self, z):\n        h = z\n        for l in self.dec_layers: h = l(h)\n        return self.out_layer(h)\n    def sample(self, mu, logvar):\n        return mu + tf.exp(0.5 * logvar) * tf.random.normal(tf.shape(mu))\n    def call(self, x, training=False):\n        mu, logvar = self.encode(x, training=training)\n        z  = self.sample(mu, logvar)\n        return self.decode(z)\n    def train_step(self, data):\n        x = data[0] if isinstance(data, tuple) else data\n        with tf.GradientTape() as tape:\n            mu, logvar = self.encode(x, training=True)\n            z  = self.sample(mu, logvar)\n            xh = self.decode(z)\n            recon = self.mse(x, xh)\n            kl = -0.5 * tf.reduce_mean(1 + logvar - tf.square(mu) - tf.exp(logvar))\n            loss = recon + self.beta * kl\n        grads = tape.gradient(loss, self.trainable_variables)\n        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n        return {\"loss\": loss, \"recon\": recon, \"kl\": kl}\n    def test_step(self, data):\n        x = data[0] if isinstance(data, tuple) else data\n        mu, logvar = self.encode(x, training=False)\n        z  = self.sample(mu, logvar)\n        xh = self.decode(z)\n        recon = self.mse(x, xh)\n        kl = -0.5 * tf.reduce_mean(1 + logvar - tf.square(mu) - tf.exp(logvar))\n        return {\"loss\": recon + self.beta * kl, \"recon\": recon, \"kl\": kl}\n    def encode_mu(self, x):\n        x = tf.convert_to_tensor(x, dtype=tf.float32)\n        mu, _ = self.encode(x, training=False)\n        return mu\n\ndef build_vae(input_dim, beta=1.0, noise_std=0.01):\n    vae = VAE(input_dim, HID, LATENT, beta=beta, noise_std=noise_std)\n    vae.compile(optimizer=get_optimizer(1e-3))\n    return vae\n\nclass BetaWarmup(tf.keras.callbacks.Callback):\n    def __init__(self, model, beta_start, beta_end, warmup_epochs):\n        super().__init__(); self.m = model\n        self.b0 = beta_start; self.b1 = beta_end; self.w = warmup_epochs\n    def on_epoch_begin(self, epoch, logs=None):\n        t = min(1.0, (epoch + 1) / float(self.w))\n        self.m.beta = self.b0 + t * (self.b1 - self.b0)\n\ndef build_export_infer(vae_model, input_dim):\n    xin = tf.keras.Input(shape=(input_dim,), name=\"input_features\")\n    h   = xin\n    for layer in vae_model.enc_layers: h = layer(h)\n    mu  = vae_model.mu(h)\n    xout = vae_model.decode(mu)\n    return tf.keras.Model(xin, xout, name=\"vae_mu_decoder_infer\")\n\n\ndef evaluate_model(model_name, plot_tag, model, Xte_g, Xf, Xg_all_sampled, latent_fn):\n    err_g = recon_errors(model, Xte_g, deterministic=True)\n    err_f = recon_errors(model, Xf,    deterministic=True)\n\n    # thresholds\n    thr_p = thr_percentile(err_g)\n    thr_e = thr_evt(err_g)\n\n    y_true = np.concatenate([np.zeros_like(err_g), np.ones_like(err_f)])\n    scores = np.concatenate([err_g, err_f])\n    auroc  = roc_auc_score(y_true, scores)\n    aupr   = average_precision_score(y_true, scores)\n\n    plot_hist(err_g, err_f, f\"{plot_tag}_hist\", pmax=99.5, use_log=False)\n    plot_hist(err_g, err_f, f\"{plot_tag}_hist_log\", pmax=99.9, use_log=True)\n\n    plot_roc_pr(y_true, scores, f\"{plot_tag}\")\n\n    uncert = None\n    if model_name in (\"SEN\", \"VAE\"):\n        Ss = [recon_errors(model, Xf, deterministic=False) for _ in range(MC_RUNS)]\n        uncert = float(np.mean(np.std(Ss, 0)))\n\n    mu_g = latent_fn(Xg_all_sampled.astype(np.float32)).numpy()\n    mu_f = latent_fn(sample_for_manifold(Xf).astype(np.float32)).numpy()\n    tw   = float(trustworthiness(Xg_all_sampled, mu_g, n_neighbors=N_NEIGH))\n    cont = float(trustworthiness(mu_g, Xg_all_sampled, n_neighbors=N_NEIGH))\n    evr  = plot_latent(mu_g, mu_f, plot_tag)\n\n    npz_path = os.path.join(OUT_DIR, f\"latent_pca3_{plot_tag}.npz\")\n    save_latent_pca3_npz(mu_g, mu_f, npz_path)\n\n    cm = confusion_matrix(y_true, (scores >= thr_p).astype(int))\n    tn, fp, fn, tp = cm.ravel()\n    prec  = tp / (tp + fp + 1e-9)\n    recall= tp / (tp + fn + 1e-9)\n    f1    = 2 * prec * recall / (prec + recall + 1e-9)\n\n    return dict(\n        model=model_name, auroc=float(auroc), aupr=float(aupr),\n        thr_percentile=thr_p, thr_evt=thr_e,\n        trustworthiness=tw, continuity=cont,\n        uncert_fault=uncert,\n        precision=float(prec), recall=float(recall), f1=float(f1),\n        cm=cm.tolist(),\n        pca_explained_variance_ratio=evr.tolist(),\n        pca_npz=npz_path\n    )\n\nif __name__ == \"__main__\":\n    Xg, Xf, scaler_info = load_and_scale(noise_std=0.005, use_robust=True)\n\n    n = len(Xg)\n    i_tr, i_va = int(0.6 * n), int(0.8 * n)\n    Xtr, Xva, Xte_g = Xg[:i_tr], Xg[i_tr:i_va], Xg[i_va:]\n    input_dim = Xtr.shape[1]\n    Xg_s = sample_for_manifold(Xte_g)\n    all_seed_results = []\n\n    for s in SEEDS:\n        print(f\"\\n================= SEED {s} =================\")\n        set_seed(s)\n        train_ds = make_ds(Xtr.astype(np.float32), BATCH, True, seed=s)\n        val_ds   = make_ds(Xva.astype(np.float32),  BATCH, False, seed=s)\n        results_this_seed = []\n\n        if ENABLED.get(\"AE\", False):\n            ae = build_ae(input_dim)\n            _ = ae(tf.zeros((1, input_dim), dtype=tf.float32))\n            pretty_stats(\"AE\", input_dim, HID, LATENT, keras_model=ae)\n            ae.fit(train_ds, validation_data=val_ds, epochs=EPOCHS_AE, verbose=2,\n                   callbacks=[tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=6, restore_best_weights=True)])\n            res_ae = evaluate_model(\"AE\", f\"AE_seed{s}\", ae, Xte_g, Xf, Xg_s,\n                                    latent_fn=lambda X: ae.encode_mu(X))\n            results_this_seed.append(res_ae)\n            if SAVE_WEIGHTS.get(\"AE\", False):\n                ae.save_weights(os.path.join(OUT_DIR, f\"ae_seed{s}.weights.h5\"))\n            del ae; tf.keras.backend.clear_session(); gc.collect(); set_seed(s)\n\n\n        if ENABLED.get(\"SEN\", False):\n            sen = build_sen(input_dim, noise_std=SEN_NOISE_STD)\n            _ = sen(tf.zeros((1, input_dim), dtype=tf.float32))\n            pretty_stats(\"SEN\", input_dim, HID, LATENT, keras_model=sen)\n            sen.fit(train_ds, validation_data=val_ds, epochs=EPOCHS_SEN, verbose=2,\n                    callbacks=[tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=6, restore_best_weights=True)])\n            res_sen = evaluate_model(\"SEN\", f\"SEN_seed{s}\", sen, Xte_g, Xf, Xg_s,\n                                     latent_fn=lambda X: sen.encode_mu(X))\n            results_this_seed.append(res_sen)\n            if SAVE_WEIGHTS.get(\"SEN\", False):\n                sen.save_weights(os.path.join(OUT_DIR, f\"sen_seed{s}.weights.h5\"))\n            del sen; tf.keras.backend.clear_session(); gc.collect(); set_seed(s)\n\n        if ENABLED.get(\"VAE\", True):\n            vae = build_vae(input_dim, beta=BETA_END, noise_std=0.01)\n            _ = vae(tf.zeros((1, input_dim), dtype=tf.float32))\n            pretty_stats(\"VAE\", input_dim, HID, LATENT, keras_model=vae)\n            vae.fit(\n                train_ds, validation_data=val_ds, epochs=EPOCHS_VAE, verbose=2,\n                callbacks=[\n                    BetaWarmup(vae, BETA_START, BETA_END, WARMUP_EPOCHS),\n                    tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=8, restore_best_weights=True)\n                ]\n            )\n            res_vae = evaluate_model(\"VAE\", f\"VAE_seed{s}\", vae, Xte_g, Xf, Xg_s,\n                                     latent_fn=lambda X: vae.encode_mu(X))\n            results_this_seed.append(res_vae)\n            if SAVE_WEIGHTS.get(\"VAE\", True):\n                vae.save_weights(os.path.join(OUT_DIR, f\"vae_seed{s}.weights.h5\"))\n            del vae; tf.keras.backend.clear_session(); gc.collect(); set_seed(s)\n\n        all_seed_results.append(dict(seed=s, results=results_this_seed))\n\n    rows = []\n    for item in all_seed_results:\n        for r in item[\"results\"]:\n            rows.append(dict(seed=item[\"seed\"], **r))\n\n    if len(rows) == 0:\n        print(\"No models enabled; nothing to aggregate.\")\n        raise SystemExit(0)\n\n    df = pd.DataFrame(rows)\n    df.to_csv(os.path.join(OUT_DIR, \"results_all_seeds.csv\"), index=False)\n\n    agg = df.groupby(\"model\").agg(\n        auroc_mean=(\"auroc\",\"mean\"), auroc_std=(\"auroc\",\"std\"),\n        aupr_mean=(\"aupr\",\"mean\"), aupr_std=(\"aupr\",\"std\"),\n        f1_mean=(\"f1\",\"mean\"), f1_std=(\"f1\",\"std\"),\n        precision_mean=(\"precision\",\"mean\"), precision_std=(\"precision\",\"std\"),\n        recall_mean=(\"recall\",\"mean\"), recall_std=(\"recall\",\"std\"),\n        trustworthiness_mean=(\"trustworthiness\",\"mean\"), trustworthiness_std=(\"trustworthiness\",\"std\"),\n        continuity_mean=(\"continuity\",\"mean\"), continuity_std=(\"continuity\",\"std\"),\n        uncert_fault_mean=(\"uncert_fault\",\"mean\"), uncert_fault_std=(\"uncert_fault\",\"std\")\n    ).reset_index()\n    agg.to_csv(os.path.join(OUT_DIR, \"results_summary.csv\"), index=False)\n\n    with open(os.path.join(OUT_DIR, \"results_all.json\"), \"w\") as f:\n        json.dump(all_seed_results, f, indent=2)\n    with open(os.path.join(OUT_DIR, \"results_summary.json\"), \"w\") as f:\n        json.dump(agg.to_dict(orient=\"records\"), f, indent=2)\n\n    if ENABLED.get(\"VAE\", True) and EXPORT_VAE:\n        df_vae = df[df.model==\"VAE\"]\n        if len(df_vae) == 0:\n            print(\"VAE not present in results; skipping export.\")\n        else:\n            best_seed = df_vae.sort_values(\"auroc\", ascending=False).iloc[0][\"seed\"]\n            print(\"Best seed for VAE export:\", best_seed)\n\n            set_seed(int(best_seed))\n            Xtr, Xva = Xtr.astype(np.float32), Xva.astype(np.float32)\n            Xtr_full = np.vstack([Xtr, Xva])\n            train_full_ds = make_ds(Xtr_full, BATCH, True, seed=int(best_seed))\n\n            vae_final = build_vae(input_dim, beta=BETA_END, noise_std=0.01)\n            _ = vae_final(tf.zeros((1, input_dim), dtype=tf.float32))\n            vae_final.fit(\n                train_full_ds, epochs=EPOCHS_VAE, verbose=2,\n                callbacks=[\n                    BetaWarmup(vae_final, BETA_START, BETA_END, WARMUP_EPOCHS),\n                    tf.keras.callbacks.EarlyStopping(monitor=\"loss\", patience=6, restore_best_weights=True)\n                ]\n            )\n\n            if isinstance(scaler_info, dict):\n                np.savez(os.path.join(OUT_DIR, \"scaler_min_max.npz\"), **scaler_info)\n            else:\n                joblib.dump(scaler_info, os.path.join(OUT_DIR, \"robust_scaler.pkl\"))\n\n            infer = build_export_infer(vae_final, input_dim)\n            infer.save(os.path.join(OUT_DIR, \"vae_fp32.keras\"))\n\n            def rep_ds():\n                Xrep = Xtr_full\n                for i in range(0, min(20000, len(Xrep)), 256):\n                    yield [Xrep[i:i+256]]\n\n            conv = tf.lite.TFLiteConverter.from_keras_model(infer)\n            conv.optimizations = [tf.lite.Optimize.DEFAULT]\n            conv.representative_dataset = rep_ds\n            conv.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n            conv.inference_input_type  = tf.int8\n            conv.inference_output_type = tf.int8\n            tfl = conv.convert()\n            tfl_path = os.path.join(OUT_DIR, \"vae_int8.tflite\")\n            with open(tfl_path, \"wb\") as f:\n                f.write(tfl)\n\n            best_row = df_vae[df_vae.seed==best_seed].iloc[0]\n            with open(os.path.join(OUT_DIR, \"deployment_thresholds.json\"), \"w\") as f:\n                json.dump({\n                    \"best_seed\": int(best_seed),\n                    \"thr_percentile\": float(best_row[\"thr_percentile\"]),\n                    \"thr_evt\": float(best_row[\"thr_evt\"])\n                }, f, indent=2)\n\n            pretty_stats(\"VAE final INT8\", input_dim, HID, LATENT, tflite_path=tfl_path)\n            print(\"Exported model + scaler + thresholds to\", OUT_DIR)\n    else:\n        print(\"VAE export disabled or VAE not enabled; skipping export.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}